# Claude to Azure Proxy Configuration
# Copy this file to .env and update with your actual values

# Server Configuration
PORT=8080
NODE_ENV=development

# Proxy API Key (used to authenticate requests to this proxy)
PROXY_API_KEY=your-secure-proxy-api-key-here

# Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here

# Azure OpenAI Available Models (comma-separated)
# Specify one or more model IDs that should be available via the API
# Model Descriptions:
#   gpt-4o: Flagship multimodal model (text/images/audio), 128K context, real-time interactions
#   gpt-4o-mini: Cost-efficient small model, 60% cheaper, 128K context, high-volume tasks
#   gpt-4.1: Latest GPT-4 with 1M context, 54.6% SWE-bench, improved coding & instruction following
#   gpt-5: State-of-the-art coding (74.9% SWE-bench, 88% Aider), 400K context, agentic workflows
#   gpt-5-codex: Specialized coding variant, custom tools, IDE integrations
#   gpt-5-pro: Premium with enhanced reasoning, 80% fewer errors, health/legal analysis
#   gpt-4-turbo: Advanced GPT-4, 128K context, Dall-E 3 integration
#   o1-preview: Advanced reasoning for complex problems
#   o1-mini: Efficient reasoning for coding and math
# Examples:
#   General use: AZURE_OPENAI_MODEL=gpt-4o,gpt-4o-mini
#   Coding focus: AZURE_OPENAI_MODEL=gpt-5,gpt-5-codex,gpt-4.1,gpt-4o
#   Advanced reasoning: AZURE_OPENAI_MODEL=gpt-5-pro,gpt-4.1,o1-preview
# Leave empty to disable Azure OpenAI models
AZURE_OPENAI_MODEL=gpt-5,gpt-5-codex,gpt-4o,gpt-4.1,gpt-5-pro

# Azure OpenAI Responses API Configuration
# Note: API version is automatically handled by the latest stable API
AZURE_OPENAI_TIMEOUT=120000
AZURE_OPENAI_MAX_RETRIES=3

# Reasoning Configuration
DEFAULT_REASONING_EFFORT=medium

# Security Configuration
ENABLE_CONTENT_SECURITY_VALIDATION=true

# Optional: Rate Limiting Configuration
# RATE_LIMIT_WINDOW_MS=900000
# RATE_LIMIT_MAX_REQUESTS=100

# Optional: CORS Configuration
# CORS_ORIGIN=*
# CORS_CREDENTIALS=false

# Optional: Logging Configuration
# LOG_LEVEL=info
# LOG_FORMAT=json

# Optional: AWS Bedrock Configuration (enables Qwen model support)
# Uncomment and configure these variables to enable AWS Bedrock integration
# AWS_BEDROCK_API_KEY=your-aws-bedrock-api-key-here
# AWS_BEDROCK_REGION=us-west-2

# AWS Bedrock Available Models (comma-separated, optional)
# Specify one or more Bedrock model IDs that should be available via the API
# Model Descriptions:
#   qwen.qwen3-coder-480b-a35b-v1:0: Flagship MoE coding model
#     - 480B total params, 35B active (Mixture-of-Experts architecture)
#     - 256K native context, extensible to 1M tokens
#     - Optimized for agentic coding, tool orchestration, repository analysis
#     - Supports Python, JavaScript, Java, C++, Go, Rust
#     - Advanced function calling and browser automation
#     - Ideal for autonomous coding agents and complex software engineering
#     - Multilingual support (29+ languages, strong Chinese/English)
# Examples:
#   Single model: AWS_BEDROCK_MODELS=qwen.qwen3-coder-480b-a35b-v1:0
# Leave empty or omit to disable AWS Bedrock models
# AWS_BEDROCK_MODELS=qwen.qwen3-coder-480b-a35b-v1:0

# AWS_BEDROCK_TIMEOUT=120000
# AWS_BEDROCK_MAX_RETRIES=3

# Model Routing Configuration
# The proxy automatically routes requests based on the model parameter:
# - qwen-3-coder, qwen.qwen3-coder-480b-a35b-v1:0 → AWS Bedrock (requires AWS_BEDROCK_API_KEY)
# - gpt-5-codex, gpt-4, claude-3-5-sonnet-20241022 → Azure OpenAI
# - Default fallback → Azure OpenAI
#
# Validation Commands:
# Test Azure OpenAI: curl -H "Authorization: Bearer $AZURE_OPENAI_API_KEY" "$AZURE_OPENAI_ENDPOINT/openai/v1/models"
# Test AWS Bedrock: curl -X POST "https://bedrock-runtime.$AWS_BEDROCK_REGION.amazonaws.com/model/qwen.qwen3-coder-480b-a35b-v1:0/converse" -H "Authorization: Bearer $AWS_BEDROCK_API_KEY" -H "Content-Type: application/json" -d '{"messages":[{"role":"user","content":[{"text":"test"}]}],"inferenceConfig":{"maxTokens":10}}'
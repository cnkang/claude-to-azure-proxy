# Claude to Azure Proxy Configuration
# Copy this file to .env and update with your actual values

# Server Configuration
PORT=8080
NODE_ENV=development

# Proxy API Key (used to authenticate requests to this proxy)
PROXY_API_KEY=your-secure-proxy-api-key-here

# Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
AZURE_OPENAI_MODEL=your-model-deployment-name

# Azure OpenAI Responses API Configuration
# AZURE_OPENAI_API_VERSION=preview  # Optional: only set for legacy preview API (leave empty for GA v1)
AZURE_OPENAI_TIMEOUT=120000
AZURE_OPENAI_MAX_RETRIES=3

# Reasoning Configuration
DEFAULT_REASONING_EFFORT=medium

# Security Configuration
ENABLE_CONTENT_SECURITY_VALIDATION=true

# Optional: Rate Limiting Configuration
# RATE_LIMIT_WINDOW_MS=900000
# RATE_LIMIT_MAX_REQUESTS=100

# Optional: CORS Configuration
# CORS_ORIGIN=*
# CORS_CREDENTIALS=false

# Optional: Logging Configuration
# LOG_LEVEL=info
# LOG_FORMAT=json

# Optional: AWS Bedrock Configuration (enables Qwen model support)
# Uncomment and configure these variables to enable AWS Bedrock integration
# This allows routing requests to Qwen models (qwen-3-coder, qwen.qwen3-coder-480b-a35b-v1:0)
# AWS_BEDROCK_API_KEY=your-aws-bedrock-api-key-here
# AWS_BEDROCK_REGION=us-west-2
# AWS_BEDROCK_TIMEOUT=120000
# AWS_BEDROCK_MAX_RETRIES=3

# Model Routing Configuration
# The proxy automatically routes requests based on the model parameter:
# - qwen-3-coder, qwen.qwen3-coder-480b-a35b-v1:0 → AWS Bedrock (requires AWS_BEDROCK_API_KEY)
# - gpt-5-codex, gpt-4, claude-3-5-sonnet-20241022 → Azure OpenAI
# - Default fallback → Azure OpenAI
#
# Validation Commands:
# Test Azure OpenAI: curl -H "Authorization: Bearer $AZURE_OPENAI_API_KEY" "$AZURE_OPENAI_ENDPOINT/openai/v1/models"
# Test AWS Bedrock: curl -X POST "https://bedrock-runtime.$AWS_BEDROCK_REGION.amazonaws.com/model/qwen.qwen3-coder-480b-a35b-v1:0/converse" -H "Authorization: Bearer $AWS_BEDROCK_API_KEY" -H "Content-Type: application/json" -d '{"messages":[{"role":"user","content":[{"text":"test"}]}],"inferenceConfig":{"maxTokens":10}}'